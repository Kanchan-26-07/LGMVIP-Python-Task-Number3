# LGMVIP-Python-Task-Number3
In this situation, tokenizing helps to fragment large text datasets into small, readable chunks (like words). Afterwards, you can also lemmatize a word, which transforms it into its lemma form. Afterwards, it creates a pickle file to store the Python objects used to predict the botâ€™s responses.
